{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenFEMA API Tutorial: Part 3 - Paging to get Data\n",
    "  \n",
    "## Quick Summary\n",
    "- This tutorial demonstrates the use of a \"paging\" technique to retrieve data when more than 1,000 records are found - *it is not a Python language tutorial*\n",
    "- The OpenFEMA API parameters \\\\$skip and \\\\$top are used in conjunction with the record count to iterate through the data\n",
    "- A [Final Working Example](#Final-Working-Example) demonstrates the paging technique in Python (our GitHub repository contains examples in other languages)\n",
    "- The next tutorial will cover Getting Dataset Updates\n",
    "\n",
    "## Overview\n",
    "In previous tutorials we demonstrated the basic use of the OpenFEMA API and parameters to alter the content and delivery of  data - the fields to return, the sort order, and *what* records to return based on criteria. Other parameters affect data delivery or *how* the data is returned - file format, file names, and the number of records to return at one time. Because OpenFEMA limits the maximum number of records returned per call to 1,000, we need to issue multiple calls using some of these parameters. \n",
    " \n",
    "The purpose of this notebook is to demonstrate how to retrieve more than 1,000 found records using a technique called \"paging\" that utilizes the \\\\$skip parameter, $top parameter, and the \"count\" value.The examples are presented using Python 3, but it should be easy to translate them to almost any programming language.\n",
    "\n",
    "See the [OpenFEMA Documentation](https://www.fema.gov/about/openfema/api) for details on the API parameters.\n",
    "\n",
    "See the [OpenFEMA Code Samples GitHub Repository](https://github.com/FEMA/openfema-samples) for paging examples in various languages.\n",
    "\n",
    "## Paging Through Data\n",
    "For performance reasons, only 1,000 records are returned per API endpoint call. If more than 1,000 records exist, it will be necessary to page through the data to retrieve every record. The metadata header returned as part of the data set JSON response will only display the full record count if the \\\\$inlinecount parameter is usedâ€”otherwise, it will have a value of 0. Computer code containing a loop is written to continue making API calls, incrementing the \\\\$skip parameter accordingly until the number of records retrieved equals the total record count.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> Several of the GitHub code examples download CSV files. It is recommended that results be downloaded in a JSON format. This format is native to the OpenFEMA data store, as such data need not be converted by the server thereby improving download performance.\n",
    "</div>\n",
    "\n",
    "### Defining and Calling the API Query\n",
    "Define the endpoint and any parameters. As explained in parts 1 and 2 of the tutorial, the process to call an API endpoint is straightforward - declare a library or module that can facilitate requests to web resources, define the URL/API endpoint, specify parameters, and issue the API call.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> \n",
    "    <ul>\n",
    "    <li>If you need the full, unfiltered file, use one of the file download links on the dataset page.\n",
    "    <li>For very large files, try to limit the data with the \\$filter and \\$select parameters. This is optional of course.</li>\n",
    "    <li>Specify a sort order to ensure that the proper records are returned with each call. If the sort is unimportant, use id.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "The previous tutorials used the FemaWebDisasterDeclarations endpoint. This tutorial will use the DisasterDeclarationsSummaries dataset as it contains many more records, allowing us to filter and still find more than 1,000 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a URL handling module\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# define URL for the Disaster Declarations Summaries endpoint\n",
    "baseUrl = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries\"\n",
    "\n",
    "# define a query using parameters \n",
    "select = \"?$select=disasterNumber,declarationDate,declarationTitle,state\"    # leave this parameter out if you want all fields\n",
    "filter = \"&$filter=state%20eq%20%27LA%27\"                                    # for purposes of example, limit to Louisiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make an initial call using the \\\\$inlinecount parameter. We want to know how many records match our criteria. This will tell us how many times we must call the endpoint. Why not just include this parameter in our main request for data? As explained in part 2 of the tutorial, \\\\$inlinecount forces the OpenFEMA server to count all the matching records *for each call*. If working with a large dataset (such as NFIP policies that contains more than 61 million records) where you must issue thousands of calls to retrieve all the data, including $inlinecount in every call will significantly reduce performance.\n",
    "\n",
    "For this initial call, limit it to 1 record, 1 field, and we don't care about the sort or the return format. Rather than define these parameter values in variables and combine, we will just add them to our URL and filter.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> Because we are ignoring the \\$select parameter above, we have to add back in our base URL/query separator - the question mark (?). The order in which the parameters are combined is not important.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"skip\": 0,\n",
      "  \"filter\": \"state eq 'LA'\",\n",
      "  \"orderby\": \"\",\n",
      "  \"select\": null,\n",
      "  \"rundate\": \"2022-11-07T20:57:13.882Z\",\n",
      "  \"entityname\": \"DisasterDeclarationsSummaries\",\n",
      "  \"version\": \"v2\",\n",
      "  \"top\": 1,\n",
      "  \"count\": 2493,\n",
      "  \"format\": \"json\",\n",
      "  \"metadata\": true,\n",
      "  \"url\": \"/api/open/v2/DisasterDeclarationsSummaries?$inlinecount=allpages&$select=id&$top=1&$filter=state%20eq%20%27LA%27\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "request = urllib.request.urlopen(baseUrl + \"?$inlinecount=allpages&$select=id&$top=1\" + filter )\n",
    "\n",
    "# actually read the data\n",
    "result = request.read()\n",
    "\n",
    "# transform to Python dictionary\n",
    "jsonData = json.loads(result.decode('utf-8'))\n",
    "\n",
    "# display the metadata object\n",
    "print(json.dumps(jsonData['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a total count of records matching our criteria, we can calculate the number of times we must issue the call to return all of it. We will retrieve the maximum number of records we can (1,000) so we can minimize the number of calls we must make. So, divide by our count by our record limit. Round any fractional amount up or the last call will be missed. We will use the \"math\" library to help with this.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> Because the default number of records returned is 1,000, we don't have to explicitly define it or use the $top parameter. We will define it in this example for clarity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2493 records, we will need to issue 3 calls\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# number of records we want to return with each call\n",
    "top = 1000\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "# print amounts\n",
    "print(\"For \" + str(recCount) + \" records, we will need to issue \" + str(loopNum) + \" calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define our loop and begin to issue calls, lets take care of a few more items:\n",
    "\n",
    " - Define additional parameters:\n",
    "   - \\\\$orderby - order is unimportant for the final result in this example, but we should specify one - use id\n",
    "   - \\\\$top - as mentioned above, not really necessary because we want the maximum for each call, but for clarity, we will include\n",
    "   - \\\\$metadata - lets turn off, otherwise the calls will take longer and we will have to skip it. Since we will be using a jsona format, turning metadata off is optional because specifying this format will suppress the metadata. We will include it in this example for clarity.\n",
    "   - \\\\$format - use jsona instead of json to simplify \n",
    " - Open a file so that we can save our data. While this is optional (you could save the data in a variable and then proceed with some sort of manipulation or analysis), the most common use case is to save the data so it can be easily retrieved and used later.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> Earlier, we defined the call return type to be jsona. Why? If we use the json format, every call will result in a top level json object (named after the dataset endpoint) containing an array of records. The top level object will have to be skipped for each iteration - all we want are the records. While this is not hard to do in Python, specifying jsona gives us just an array of records.\n",
    "    \n",
    "A side effect of this technique is that the final file (of just saved records) will not be recognized as a valid json file for future use (because it really is a json array file). This can be overcome by adding the high level object back in to the file once.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> A csv format may be specified if desired. The code in these examples will have to change slightly. The code examples in the OpenFEMA GitHub repository has examples. \n",
    "    \n",
    "There is a tradeoff with regards to specifying a csv format. Less data must be transmitted because the format is not as verbose, but the OpenFEMA server must work to convert the data from json to csv - potentially increasing the download time. It is also possible to convert the format after it has been downloaded.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderby = \"&$orderby=id\"     # order unimportant to me, so use id\n",
    "limit = \"&$top=1000\"         # not needed as the default is 1000 - including for clarity\n",
    "format = \"&$format=jsona\"    # lets use an array of json objects - easier\n",
    "other = \"&$metadata=off\"     # not needed as jsona suppresses metadata - including for clarity\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"dds_output.json\", \"a\")\n",
    "outFile.write('{\"disasterdeclarationssummaries\":[');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make subsequent calls until all our data has been fetched. With each iteration, skip the records that have already been fetched.\n",
    "\n",
    " - Compose a request, skipping those already fetched\n",
    " - Continue issuing calls until we have reached our loop count\n",
    " - Determine if we are on the last call to terminate the json object if necessary\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> An alternative to saving the data after each call is to accumulate the results in a variable and then save it all at once. This can be faster. This may not be possible due to memory limitations for extremely large datasets.\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n"
     ]
    }
   ],
   "source": [
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "skip = 0\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    request = urllib.request.urlopen(baseUrl + select + filter + orderby + limit + format + other + \"&$skip=\" + str(skip))\n",
    "    result = request.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all the data has been retrieved and saved. We can verify that all the records were retrieved by opening the file, loading the data, and counting the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493 records in file\n"
     ]
    }
   ],
   "source": [
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"dds_output.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(str(len(my_data['disasterdeclarationssummaries'])) + \" records in file\")\n",
    "inFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications for CSV Format\n",
    "The following illustrates saving data in a csv format. We changed the \\\\$format parameter and removed the code to make the result a proper json file. It is necessary to check whether we are on the first pull to ensure we output the field headers only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n"
     ]
    }
   ],
   "source": [
    "orderby = \"&$orderby=id\"     # order unimportant to me, so use id\n",
    "limit = \"&$top=1000\"         # not needed as the default is 1000 - including for clarity\n",
    "format = \"&$format=csv\"      # lets use csv as our output type\n",
    "other = \"&$metadata=off\"     # not needed as csv suppresses metadata - including for clarity\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"dds_output.csv\", \"a\")\n",
    "\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "skip = 0\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # issue call, decode the data, and save to a file\n",
    "    request = urllib.request.urlopen(baseUrl + select + filter + orderby + limit + format + other + \"&$skip=\" + str(skip))\n",
    "    result = request.read()\n",
    "    csvData = result.decode('utf-8')\n",
    "    \n",
    "    # avoid writing the header/fieldnames every time\n",
    "    if (i == 0):\n",
    "        # on the first record, so write full output that includes field headers\n",
    "        outFile.write(csvData)\n",
    "    else:\n",
    "        # split off the first row\n",
    "        outFile.write(csvData.split(\"\\n\",1)[1])\n",
    "    \n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Working Example\n",
    "Although contrived, this final example applies many of the parameters discussed in this tutorial.  Examples in other languages can be found in the [OpenFEMA Samples on GitHub](https://github.com/FEMA/openfema-samples) repository.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> This is not meant to be a Python language tutorial. The point is to show the use and combination of OpenFEMA API parameters. There are other, more Pythonic ways that this can be done. If you are writing production quality code, it is recommended that you follow industry best practices - evaluate returned values, add error handling, add logging, proper object cleanup, build for resilience by adding retries if failure, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metadata\": {\n",
      "    \"skip\": 10,\n",
      "    \"filter\": \"stateCode eq 'VA'\",\n",
      "    \"orderby\": \"declarationDate DESC\",\n",
      "    \"select\": null,\n",
      "    \"rundate\": \"2022-10-04T15:55:07.820Z\",\n",
      "    \"entityname\": \"FemaWebDisasterDeclarations\",\n",
      "    \"version\": \"v1\",\n",
      "    \"top\": 3,\n",
      "    \"count\": 73,\n",
      "    \"format\": \"json\",\n",
      "    \"metadata\": true,\n",
      "    \"url\": \"/api/open/v1/FemaWebDisasterDeclarations?$select=disasterNumber,declarationDate,disasterName,stateCode&$filter=stateCode%20eq%20%27VA%27&$orderby=declarationDate%20desc&$skip=10&$top=3&$format=json&$inlinecount=allpages\"\n",
      "  },\n",
      "  \"FemaWebDisasterDeclarations\": [\n",
      "    {\n",
      "      \"disasterNumber\": 4262,\n",
      "      \"declarationDate\": \"2016-03-07T00:00:00.000Z\",\n",
      "      \"disasterName\": \"SEVERE WINTER STORM AND SNOWSTORM\",\n",
      "      \"stateCode\": \"VA\"\n",
      "    },\n",
      "    {\n",
      "      \"disasterNumber\": 4092,\n",
      "      \"declarationDate\": \"2012-11-26T00:00:00.000Z\",\n",
      "      \"disasterName\": \"HURRICANE SANDY\",\n",
      "      \"stateCode\": \"VA\"\n",
      "    },\n",
      "    {\n",
      "      \"disasterNumber\": 3359,\n",
      "      \"declarationDate\": \"2012-10-29T00:00:00.000Z\",\n",
      "      \"disasterName\": \"HURRICANE SANDY\",\n",
      "      \"stateCode\": \"VA\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# declare a URL handling module\n",
    "import urllib.request\n",
    "import json\n",
    "import math\n",
    "\n",
    "# define URL for the Disaster Declarations Summaries endpoint\n",
    "baseUrl = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries\"\n",
    "\n",
    "# define a query using parameters \n",
    "select = \"?$select=disasterNumber,declarationDate,declarationTitle,state\"    # leave this parameter out if you want all fields\n",
    "filter = \"&$filter=state%20eq%20%27LA%27\"                                    # for purposes of example, limit to Louisiana\n",
    "orderby = \"&$orderby=id\"     # order unimportant to me, so use id\n",
    "limit = \"&$top=1000\"         # not needed as the default is 1000 - including for clarity\n",
    "format = \"&$format=jsona\"    # lets use an array of json objects - easier\n",
    "other = \"&$metadata=off\"     # not needed as jsona suppresses metadata - including for clarity\n",
    "\n",
    "# number of records we want to return with each call\n",
    "top = 1000\n",
    "\n",
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "request = urllib.request.urlopen(baseUrl + \"?$inlinecount=allpages&$select=id&$top=1\" + filter )\n",
    "\n",
    "# actually read the data\n",
    "result = request.read()\n",
    "\n",
    "# transform to Python dictionary\n",
    "jsonData = json.loads(result.decode('utf-8'))\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"dds_output.json\", \"a\")\n",
    "outFile.write('{\"disasterdeclarationssummaries\":[');\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "skip = 0\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    request = urllib.request.urlopen(baseUrl + select + filter + orderby + limit + format + other + \"&$skip=\" + str(skip))\n",
    "    result = request.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "outFile.close()\n",
    "\n",
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"dds_output.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(str(len(my_data['disasterdeclarationssummaries'])) + \" records in file\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go Next\n",
    "OpenFEMA API_Tutorial_Part_4_GettingDatasetUpdates\n",
    "\n",
    "## Other Resources\n",
    "- [OpenFEMA Homepage](https://www.fema.gov/open)\n",
    "- [OpenFEMA API Documentation](https://www.fema.gov/about/openfema/api)\n",
    "- [OpenFEMA Samples on GitHub](https://github.com/FEMA/openfema-samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
